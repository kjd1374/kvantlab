import os
import json
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv
from supabase import create_client, Client
import google.generativeai as genai

# Load environment variables
load_dotenv()

# Configuration
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://hgxblbbjlnsfkffwvfao.supabase.co")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Initialize clients
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

async def stage_1_extract_trends(input_text: str) -> Dict[str, Any]:
    """
    Stage 1: Trend Analyzer
    Extracts core trends and keywords from input text.
    """
    prompt = f"""
ë„ˆëŠ” í•œêµ­ì˜ ë·°í‹°, íŒ¨ì…˜, ë¼ì´í”„ìŠ¤íƒ€ì¼ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ì—¬ ë™ë‚¨ì•„ì‹œì•„ ì…€ëŸ¬ë“¤ì—ê²Œ ê³µê¸‰í•  ìµœì ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ ë¶„ì„ê°€ì•¼.

ì•„ë˜ ì œê³µë˜ëŠ” [í…ìŠ¤íŠ¸ ë°ì´í„°]ë¥¼ ì½ê³  ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜:
1. í•µì‹¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (Trend_Keyword)
2. ì£¼ëª©í•´ì•¼ í•  í•µì‹¬ ì„±ë¶„ ë˜ëŠ” ìŠ¤íƒ€ì¼ (Key_Elements): ë°°ì—´ í˜•ì‹
3. ì´ íŠ¸ë Œë“œê°€ ìœ í–‰í•˜ëŠ” ì´ìœ  (Reason)
4. ì£¼ìš” íƒ€ê²Ÿ ì—°ë ¹ì¸µ ë° ì„±ë³„ (Target_Audience)
5. ì…€ëŸ¬ë“¤ì´ ì´ íŠ¸ë Œë“œë¥¼ í™ë³´í•  ë•Œ ì‚¬ìš©í•  í•µì‹¬ ìŠ¬ë¡œê±´ (Slogan)

[í…ìŠ¤íŠ¸ ë°ì´í„°]:
{input_text}

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œ ë¸”ë¡ë§Œ ì¶œë ¥í•´ì¤˜. JSON í‚¤ëŠ” ë°˜ë“œì‹œ ìœ„ì— ëª…ì‹œëœ ì˜ì–´ ì´ë¦„ì„ ì‚¬ìš©í•´ì¤˜.
    """
    
    response = model.generate_content(prompt)
    try:
        # Extract JSON from response
        text = response.text.strip()
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
        
        return json.loads(text)
    except Exception as e:
        print(f"Error in Stage 1: {e}")
        return {}

async def stage_2_match_products(trends: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Stage 2: The Matcher
    Matches extracted trends with products using semantic search.
    """
    keyword = trends.get("Trend_Keyword", "")
    elements = ", ".join(trends.get("Key_Elements", []))
    search_query = f"{keyword} {elements}"
    
    # Use RPC for semantic search (if available/implemented as per migration 007)
    # Falling back to name-based match if needed
    try:
        # Match products using vector similarity via RPC
        # Need to generate embedding first for the search_query
        # For simplicity in this script, we'll try to use the match_products RPC 
        # but since generating embeddings requires another call, we'll use a combined approach.
        
        # 1. Keyword search as fallback/supplement
        res = supabase.table("products_master").select("*").or_(f"name.ilike.%{keyword}%,brand.ilike.%{keyword}%").limit(10).execute()
        potential_products = res.data
        
        # 2. Use LLM to score and select top 5
        product_list_str = "\n".join([f"- ID: {p['id']}, Name: {p['name']}, Brand: {p['brand']}, Price: {p['price']}" for p in potential_products])
        
        prompt = f"""
ë„ˆëŠ” ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ìì•¼. ì£¼ì–´ì§„ [íŠ¸ë Œë“œ í‚¤ì›Œë“œ]ì™€ [ìƒí’ˆ ë¦¬ìŠ¤íŠ¸]ë¥¼ ë¹„êµí•´ì„œ, í•´ë‹¹ íŠ¸ë Œë“œì— ê°€ì¥ ì í•©í•œ ìƒí’ˆ 5ê°œë¥¼ ì„ ì •í•´ì¤˜.

[íŠ¸ë Œë“œ í‚¤ì›Œë“œ]: {keyword}, {elements}

[ìƒí’ˆ ë¦¬ìŠ¤íŠ¸]:
{product_list_str}

ë¶„ì„ ê¸°ì¤€:
1. ìƒí’ˆëª…ì´ë‚˜ ì„¤ëª…ì— íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì§ì ‘ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
2. í‚¤ì›Œë“œê°€ ì—†ë”ë¼ë„ ìƒí’ˆì˜ íš¨ëŠ¥ì´ë‚˜ ìŠ¤íƒ€ì¼ì´ íŠ¸ë Œë“œì™€ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ê°€?

ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œ ì¶œë ¥):
[
  {{
    "product_id": (ID),
    "match_score": (1-100),
    "match_reason": "ì…€ëŸ¬ê°€ ë‚©ë“í•  ìˆ˜ ìˆëŠ” ê·¼ê±°"
  }},
  ...
]
ì‘ë‹µì€ ë‹¤ë¥¸ ë©”ì‹œì§€ ì—†ì´ JSONë§Œ ì¶œë ¥í•´ì¤˜.
        """
        
        response = model.generate_content(prompt)
        text = response.text.strip()
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
            
        matches = json.loads(text)
        
        # Supplement match data with product details
        for m in matches:
            p_details = next((p for p in potential_products if p['id'] == m['product_id']), {})
            m.update(p_details)
            
        return matches
    except Exception as e:
        print(f"Error in Stage 2: {e}")
        return []

async def stage_3_generate_marketing(match: Dict[str, Any], trends: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Stage 3: The Marketer
    Generates localized marketing copy.
    """
    product_info = f"Product: {match.get('name')}, Brand: {match.get('brand')}"
    trend_info = f"Trend: {trends.get('Trend_Keyword')}, Slogan: {trends.get('Slogan')}"
    
    prompt = f"""
ë„ˆëŠ” ë™ë‚¨ì•„ì‹œì•„(ë² íŠ¸ë‚¨, íƒœêµ­) ì‹œì¥ì„ ê¿°ëš«ê³  ìˆëŠ” ê¸€ë¡œë²Œ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì•¼. 
í˜„ì¬ í•œêµ­ì—ì„œ ìœ í–‰í•˜ëŠ” íŠ¸ë Œë“œì™€ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì§€ ì…€ëŸ¬ë“¤ì´ ê³ ê°ì—ê²Œ ë°”ë¡œ ë°œì†¡í•  ìˆ˜ ìˆëŠ” í™ë³´ ë¬¸êµ¬ë¥¼ ì‘ì„±í•´ì¤˜.

[ìƒí’ˆ ì •ë³´]: {product_info}
[í˜„ì¬ í•œêµ­ íŠ¸ë Œë“œ]: {trend_info}

ìš”êµ¬ì‚¬í•­:
1. íƒ€ê²Ÿ êµ­ê°€: ë² íŠ¸ë‚¨, íƒœêµ­ (ê° êµ­ê°€ë³„ ì–¸ì–´ë¡œ ì‘ì„±) ë° í•œêµ­ì–´ ë²ˆì—­ë³¸
2. í†¤ì•¤ë§¤ë„ˆ: ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ ëŠë‚Œ (ì´ëª¨ì§€ ì ê·¹ í™œìš©)
3. êµ¬ì„±:
   - ëˆˆê¸¸ì„ ì‚¬ë¡œì¡ëŠ” í—¤ë“œë¼ì¸
   - í•œêµ­ ë‚´ ì¸ê¸° ì¦ê±° (ì˜ˆ: ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ 1ìœ„ ë“±)
   - ì´ ìƒí’ˆì„ ê¼­ ì‚¬ì•¼ í•˜ëŠ” ì´ìœ  3ê°€ì§€
   - ë§ˆì§€ë§‰ì— "í•œêµ­ ì§ë°°ì†¡ ì •í’ˆ"ì„ì„ ê°•ì¡°

ì‘ë‹µ í˜•ì‹ (JSON ë°°ì—´):
[
  {{
    "language_code": "vi",
    "headline": "...",
    "popularity_proof": "...",
    "key_reasons": ["...", "...", "..."],
    "content_body": "ì „ì²´ ë¬¸êµ¬"
  }},
  {{
    "language_code": "th",
    "headline": "...",
    "popularity_proof": "...",
    "key_reasons": ["...", "...", "..."],
    "content_body": "ì „ì²´ ë¬¸êµ¬"
  }},
  {{
    "language_code": "ko",
    "headline": "...",
    "popularity_proof": "...",
    "key_reasons": ["...", "...", "..."],
    "content_body": "ì „ì²´ ë¬¸êµ¬"
  }}
]
ì‘ë‹µì€ JSONë§Œ ì¶œë ¥í•´ì¤˜.
    """
    
    response = model.generate_content(prompt)
    try:
        text = response.text.strip()
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
        return json.loads(text)
    except Exception as e:
        print(f"Error in Stage 3: {e}")
        return []

async def run_full_workflow(input_text: str, user_id: str = None):
    """
    Orchestrates the full 3-stage workflow and saves results to DB.
    """
    print("ğŸš€ Starting Trend Analysis Workflow...")
    
    # 0. Initialize run in DB
    run_res = supabase.table("trend_analysis_runs").insert({
        "input_text": input_text,
        "user_id": user_id,
        "status": "processing"
    }).execute()
    run_id = run_res.data[0]["id"]
    
    try:
        # 1. Stage 1: Extraction
        print("Stage 1: Extracting Trends...")
        trends = await stage_1_extract_trends(input_text)
        
        extracted_trend_res = supabase.table("extracted_trends").insert({
            "run_id": run_id,
            "trend_keyword": trends.get("Trend_Keyword"),
            "key_elements": trends.get("Key_Elements"),
            "reason": trends.get("Reason"),
            "target_audience": trends.get("Target_Audience"),
            "slogan": trends.get("Slogan"),
            "raw_json": trends
        }).execute()
        trend_db_id = extracted_trend_res.data[0]["id"]
        
        # 2. Stage 2: Matching
        print("Stage 2: Matching Products...")
        matches = await stage_2_match_products(trends)
        
        for idx, match in enumerate(matches):
            match_res = supabase.table("trend_product_matches").insert({
                "run_id": run_id,
                "trend_id": trend_db_id,
                "product_id": match.get("product_id"),
                "match_score": match.get("match_score"),
                "match_reason": match.get("match_reason"),
                "rank_in_run": idx + 1
            }).execute()
            match_db_id = match_res.data[0]["id"]
            
            # 3. Stage 3: Marketing
            print(f"Stage 3: Generating Marketing for Product {idx+1}...")
            marketing_contents = await stage_3_generate_marketing(match, trends)
            
            for content in marketing_contents:
                supabase.table("marketing_contents").insert({
                    "run_id": run_id,
                    "match_id": match_db_id,
                    "language_code": content.get("language_code"),
                    "headline": content.get("headline"),
                    "popularity_proof": content.get("popularity_proof"),
                    "key_reasons": content.get("key_reasons"),
                    "content_body": content.get("content_body")
                }).execute()
        
        # Update status
        supabase.table("trend_analysis_runs").update({"status": "completed"}).eq("id", run_id).execute()
        print(f"âœ… Workflow completed successfully! Run ID: {run_id}")
        return run_id
        
    except Exception as e:
        print(f"âŒ Workflow failed: {e}")
        supabase.table("trend_analysis_runs").update({"status": "failed"}).eq("id", run_id).execute()
        raise e

# Example execution
if __name__ == "__main__":
    sample_text = """
ìµœê·¼ í•œêµ­ì—ì„œëŠ” PDRN(ì—°ì–´ ì£¼ì‚¬ ì„±ë¶„)ì„ í™œìš©í•œ í™ˆì¼€ì–´ í™”ì¥í’ˆì´ í­ë°œì ì¸ ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤. 
íŠ¹íˆ 3040 ì„¸ëŒ€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê³ ê¸°ëŠ¥ì„± ì•ˆí‹°ì—ì´ì§•ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ë©´ì„œ, í”¼ë¶€ê³¼ ì‹œìˆ ì˜ íš¨ê³¼ë¥¼ ì§‘ì—ì„œ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” 
'ìŠ¤í”¼í˜'ê³¼ 'PDRN' ê²°í•© ìƒí’ˆë“¤ì´ ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ ìƒìœ„ê¶Œì„ ì ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
ë¬´ì‹ ì‚¬ ë·°í‹°ì—ì„œë„ ë‚¨ì„±ë“¤ì„ ìœ„í•œ ê³ ê¸°ëŠ¥ì„± ì˜¬ì¸ì› ì œí’ˆë“¤ì´ íŠ¸ë Œë“œë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤.
    """
    asyncio.run(run_full_workflow(sample_text))
