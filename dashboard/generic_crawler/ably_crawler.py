"""
Ably (ì—ì´ë¸”ë¦¬) Ranking Crawler
URL: https://m.a-bly.com/

ë°©ì‹: Playwrightë¥¼ ì‚¬ìš©í•´ ëª¨ë°”ì¼ ë·°ë¡œ ì ‘ì†í•˜ì—¬ ë”¥ë§í¬/ì¹´í…Œê³ ë¦¬ íƒìƒ‰
íŠ¹ì§•: ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜(API parameter í™•ì¸) -> ë­í‚¹ ìˆ˜ì§‘
"""
import os
import json
import time
import asyncio
import requests
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright

load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
SOURCE = "ably"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("Error: SUPABASE_URL or SUPABASE_KEY not found.")
    exit(1)

HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=representation,resolution=merge-duplicates"
}

# ì—ì´ë¸”ë¦¬ ë©”ì¸ ì¹´í…Œê³ ë¦¬ (ì´ë¦„ ê¸°ë°˜ ë„¤ë¹„ê²Œì´ì…˜)
TARGET_CATEGORIES = [
    {"name": "ìƒì˜", "code": "WOMEN"},
    {"name": "ì•„ìš°í„°", "code": "WOMEN"},
    {"name": "ì›í”¼ìŠ¤", "code": "WOMEN"},
    {"name": "ë°”ì§€", "code": "WOMEN"},
    {"name": "ìŠ¤ì»¤íŠ¸", "code": "WOMEN"},
    {"name": "ê°€ë°©", "code": "BAG"},
    {"name": "ì‹ ë°œ", "code": "SHOES"},
    {"name": "ë·°í‹°", "code": "BEAUTY"}
]

def log_crawl(status, metadata=None):
    try:
        log_data = {
            "job_name": f"{SOURCE}_ranking_crawl",
            "status": status,
            "started_at": datetime.now().isoformat() if status == "running" else None,
            "finished_at": datetime.now().isoformat() if status in ("completed", "failed") else None,
            "metadata_json": metadata or {}
        }
        requests.post(f"{SUPABASE_URL}/rest/v1/crawl_logs", headers=HEADERS, json=log_data, timeout=10)
    except Exception as e:
        print(f"Warning: Could not log crawl status: {e}")

async def debug_page_structure(page, category_name):
    # ì´ í•¨ìˆ˜ëŠ” ë””ë²„ê¹… ëª©ì ìœ¼ë¡œ í˜„ì¬ í˜ì´ì§€ì˜ HTML êµ¬ì¡°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
    try:
        html_content = await page.content()
        filename = f"debug_ably_{category_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"  ğŸ” ë””ë²„ê·¸ HTML ì €ì¥: {filename}")
    except Exception as e:
        print(f"  âŒ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")

def save_product_and_rank(item, rank, category_code, category_name):
    """
    Supabaseì— ìƒí’ˆ ë° ë­í‚¹ ì •ë³´ ì €ì¥ (Upsert)
    """
    try:
        product_id = item['id']
        name = item['name']
        brand = item.get('brand_name', '')
        price = int(item['price']) if item['price'] else None
        image_url = item['image']
        url = item['url']

        # 1. products_master í…Œì´ë¸”ì— ìƒí’ˆ ì •ë³´ ì €ì¥ (Upsert)
        product_record = {
            "product_id": str(product_id),
            "source": SOURCE,
            "name": name,
            "brand": brand,
            "price": price,
            "category": category_name,
            "image_url": image_url,
            "url": url,
            "updated_at": datetime.now().isoformat()
        }

        res = requests.post(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={"on_conflict": "source,product_id"},
            json=product_record,
            timeout=10
        )

        if res.status_code not in [200, 201]:
            print(f"  âš ï¸ Product upsert error for {product_id}: {res.text[:100]}")
            return False

        # 2. ë‚´ë¶€ ID ê°€ì ¸ì˜¤ê¸°
        db_items = res.json()
        if not db_items:
            return False
        internal_id = db_items[0].get("id")

        # 3. daily_rankings_v2 í…Œì´ë¸”ì— ë­í‚¹ ì €ì¥
        ranking_record = {
            "product_id": internal_id,
            "rank": rank,
            "date": datetime.now().date().isoformat(),
            "category_code": category_code,
            "source": SOURCE
        }

        rank_res = requests.post(
            f"{SUPABASE_URL}/rest/v1/daily_rankings_v2",
            headers=HEADERS,
            params={"on_conflict": "product_id,date,category_code"},
            json=ranking_record,
            timeout=10
        )

        if rank_res.status_code not in [200, 201]:
             print(f"  âš ï¸ Rank upsert error for {product_id}: {rank_res.text[:100]}")
        
        return True

    except Exception as e:
        print(f"  âŒ Save error: {e}")
        return False


async def crawl_ably_category(page, category):
    print(f"\n--- [{category['name']}] í¬ë¡¤ë§ ì‹œì‘ (ë©”ë‰´ íƒìƒ‰) ---")
    
    # API ì‘ë‹µ ìº¡ì²˜ë¥¼ ìœ„í•œ ë³€ìˆ˜
    api_responses = []

    async def handle_response(response):
        if "api.a-bly.com" in response.url and response.status == 200:
            try:
                # JSON ì‘ë‹µë§Œ ì²˜ë¦¬
                content_type = response.headers.get("content-type", "")
                if "application/json" in content_type:
                    data = await response.json()
                    api_responses.append({
                        "url": response.url,
                        "data": data
                    })
                    # print(f"  ğŸ” API ì‘ë‹µ ìº¡ì²˜: {response.url[:60]}...")
            except:
                pass

    page.on("response", handle_response)

    # ë©”ì¸ í˜ì´ì§€ ì´ë™
    try:
        await page.goto("https://m.a-bly.com/", wait_until="networkidle", timeout=60000)
        print(f"  ğŸš© ë©”ì¸ ì ‘ê·¼: {await page.title()}")
    except Exception as e:
        print(f"  âŒ ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return 0

    await asyncio.sleep(5)
    
    # í™ˆ í˜ì´ì§€ HTML ë””ë²„ê¹…
    await debug_page_structure(page, "home_main")

    # ì¹´í…Œê³ ë¦¬ íƒ­ í´ë¦­ (í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜)
    category_clicked = False
    try:
        # í•˜ë‹¨ íƒ­ë°”: 'ì „ì²´ë³´ê¸°' ê°€ ì¹´í…Œê³ ë¦¬ ë©”ë‰´ì„ (í–„ë²„ê±° ì•„ì´ì½˜)
        # 1. í…ìŠ¤íŠ¸ 'ì „ì²´ë³´ê¸°' ë¡œ ì‹œë„
        cat_btn = page.locator("text=ì „ì²´ë³´ê¸°").first
        
        if await cat_btn.count() > 0 and await cat_btn.is_visible():
            await cat_btn.click()
            category_clicked = True
            print("  âœ… 'ì „ì²´ë³´ê¸°(ì¹´í…Œê³ ë¦¬)' íƒ­ í´ë¦­")
        else:
            # 2. ì•„ì´ì½˜(SVG)ìœ¼ë¡œ ì‹œë„ - path d ì†ì„± ì¼ë¶€ ë§¤ì¹­
            # í–„ë²„ê±° ë©”ë‰´ path: M2 6a.9.9...
            cat_svg_btn = page.locator("path[d^='M2 6a.9.9']").first
            if await cat_svg_btn.count() > 0:
                await cat_svg_btn.click()
                category_clicked = True
                print("  âœ… 'ì¹´í…Œê³ ë¦¬' ì•„ì´ì½˜ í´ë¦­")
            else:
                 # 3. í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ì˜ 2ë²ˆì§¸ ì•„ì´í…œ ê°€ì • (í™ˆ, ì „ì²´ë³´ê¸°, ê²€ìƒ‰, ë§ˆì´í˜ì´ì§€)
                 nav_items = page.locator(".sc-f21a85fc-1") # í´ë˜ìŠ¤ëª…ì€ ë³€í•  ìˆ˜ ìˆì§€ë§Œ êµ¬ì¡°ìƒ ì‹œë„
                 if await nav_items.count() >= 2:
                     await nav_items.nth(1).click()
                     category_clicked = True
                     print("  âœ… í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ 2ë²ˆì§¸ ì•„ì´í…œ í´ë¦­")
                     
    except Exception as e:
        print(f"  âš ï¸ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­ ì—ëŸ¬: {e}")

    await asyncio.sleep(2)

    if not category_clicked:
         print("  âš ï¸ ì¹´í…Œê³ ë¦¬ ì§„ì… ì‹¤íŒ¨, ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„")
         return 0

    # ìƒì„¸ ì¹´í…Œê³ ë¦¬ í´ë¦­ (Overview í˜ì´ì§€ ëŒ€ì‘)
    try:
        # íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ìƒì˜) í…ìŠ¤íŠ¸ ì°¾ê¸°
        cat_name = category['name']
        print(f"  ğŸ‘‰ '{cat_name}' ì¹´í…Œê³ ë¦¬ ì§„ì… ì‹œë„...")

        # ë¦¬ì•¡íŠ¸ íŠ¸ë¦¬ê°€ ë Œë”ë§ë  ìˆ˜ ìˆë„ë¡ ì¶©~ë¶„íˆ ê¸°ë‹¤ë ¤ì¤ë‹ˆë‹¤ (í•µì‹¬)
        await asyncio.sleep(4)

        # 1. í…ìŠ¤íŠ¸ë¡œ ìš”ì†Œ ì°¾ê¸° (í—¤ë”ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
        cat_header = page.locator(f"text={cat_name}").last 
        
        if await cat_header.count() > 0:
            await cat_header.scroll_into_view_if_needed()
            await cat_header.click()
            await asyncio.sleep(3) # í™•ì¥ ì• ë‹ˆë©”ì´ì…˜ ëŒ€ê¸°
            
            # í˜„ì¬ í˜ì´ì§€ê°€ overviewë¼ë©´ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¥¼ í´ë¦­í•´ì•¼ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ê°
            current_url = page.url
            if "overview" in current_url:
                print("  â„¹ï¸ ì¹´í…Œê³ ë¦¬ Overview í˜ì´ì§€ ê°ì§€. ì²«ë²ˆì§¸ ì„œë¸Œì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹œë„.")
                # í—¤ë”ì˜ ë¶€ëª¨(í˜¹ì€ ì¡°ìƒ)ì˜ í˜•ì œ ìš”ì†Œë¥¼ ì°¾ì•„ì•¼ í•¨.
                
                # XPath: í…ìŠ¤íŠ¸ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” píƒœê·¸ì˜ ë¶€ëª¨ divì˜ ë°”ë¡œ ë‹¤ìŒ í˜•ì œ div
                sub_cat_container_xpath = f"//p[text()='{cat_name}']/ancestor::div[1]/following-sibling::div[1]"
                
                # ê·¸ ë‚´ë¶€ì˜ ì²«ë²ˆì§¸ ì´ë¯¸ì§€(ì„œë¸Œì¹´í…Œê³ ë¦¬ ì•„ì´ì½˜)ì„ í´ë¦­
                sub_cat_first_item = page.locator(f"xpath={sub_cat_container_xpath}//img").first
                
                if await sub_cat_first_item.count() > 0:
                     print("  âœ… ì²«ë²ˆì§¸ ì„œë¸Œì¹´í…Œê³ ë¦¬ ë°œê²¬, í´ë¦­í•©ë‹ˆë‹¤.")
                     await sub_cat_first_item.click(force=True)
                     await asyncio.sleep(4) # í˜ì´ì§€ ì´ë™ ëŒ€ê¸°
                else:
                    print(f"  âš ï¸ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (XPath: {sub_cat_container_xpath})")
            else:
                 print("  info: Overview í˜ì´ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤ (ë°”ë¡œ ë¦¬ìŠ¤íŠ¸ ì§„ì… ê°€ëŠ¥ì„±).")
        else:
            print(f"  âŒ '{cat_name}' í…ìŠ¤íŠ¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
            
    except Exception as e:
         print(f"  âš ï¸ ì¹´í…Œê³ ë¦¬ ì„ íƒ ì¤‘ ì—ëŸ¬: {e}")
         return 0
    await asyncio.sleep(3)
    current_url = page.url
    print(f"  ğŸš© í˜„ì¬ URL: {current_url}")
    
    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì¸ì§€ í™•ì¸ (goods í˜¹ì€ list íŒ¨í„´)
    # Overviewì— ë¨¸ë¬¼ëŸ¬ ìˆë‹¤ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
    if "overview" in current_url:
        print("  âŒ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì§„ì… ì‹¤íŒ¨ (ì—¬ì „íˆ Overview í˜ì´ì§€)")
        return 0

    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë”© ëŒ€ê¸°
    try:
         await page.wait_for_selector("a[href*='/goods/']", timeout=10000)
    except:
         print("  âš ï¸ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹œê°„ ì´ˆê³¼ í˜¹ì€ ìƒí’ˆ ì—†ìŒ")

    # 4. ìƒí’ˆ ë­í‚¹ ìˆ˜ì§‘ (ë¬´í•œ ìŠ¤í¬ë¡¤)
    products = []
    scroll_count = 0
    # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ë” ë§ì´)
    for _ in range(5):
        await page.mouse.wheel(0, 3000)
        await asyncio.sleep(1)

    # 4. API ì‘ë‹µ ë¶„ì„ ë° ìƒí’ˆ ì¶”ì¶œ
    print(f"  ğŸ” ìº¡ì²˜ëœ API ì‘ë‹µ ìˆ˜: {len(api_responses)}")
    
    captured_products = []
    
    for res in api_responses:
        try:
            data = res['data']
            components = []
            
            # ì»´í¬ë„ŒíŠ¸ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
            if 'components' in data:
                components = data['components']
            
            if not isinstance(components, list):
                continue
                
            for i, comp in enumerate(components):
                # ë””ë²„ê¹…: ì»´í¬ë„ŒíŠ¸ í‚¤ í™•ì¸
                print(f"    [Comp {i}] Keys: {list(comp.keys())}")
                if 'wrapper' in comp: # wrapper íŒ¨í„´ ì²´í¬
                     print(f"      Wrapper Keys: {list(comp['wrapper'].keys())}")

                goods_list = []
                
                # ì»´í¬ë„ŒíŠ¸ ë‚´ë¶€ì—ì„œ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (êµ¬ì¡° ë‹¤ì–‘ì„± ëŒ€ì‘)
                # 1. comp['data']['goods']
                if 'data' in comp and isinstance(comp['data'], dict) and 'goods' in comp['data']:
                    goods_list = comp['data']['goods']
                    print(f"      âœ… Found in data.goods (Count: {len(goods_list)})")
                
                # 2. comp['entity']['goods']
                elif 'entity' in comp and isinstance(comp['entity'], dict) and 'goods' in comp['entity']:
                    goods_list = comp['entity']['goods']
                    print(f"      âœ… Found in entity.goods (Count: {len(goods_list)})")

                # 3. comp['goods']
                elif 'goods' in comp:
                    goods_list = comp['goods']
                    print(f"      âœ… Found in goods (Count: {len(goods_list)})")
                
                # 5. comp['entity']['item_list'] (New pattern found)
                elif 'entity' in comp and 'item_list' in comp['entity']:
                    item_list = comp['entity']['item_list']
                    if isinstance(item_list, list):
                        goods_list = item_list
                        print(f"      âœ… Found in entity.item_list (Count: {len(goods_list)})")
                    elif isinstance(item_list, dict) and 'goods' in item_list:
                        goods_list = item_list['goods']
                        print(f"      âœ… Found in entity.item_list.goods (Count: {len(goods_list)})")

                # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì—†ê³  entity/dataê°€ ìˆë‹¤ë©´ ê·¸ ë‚´ë¶€ í‚¤ ì¶œë ¥í•´ë³´ê¸° (ë””ë²„ê¹…ìš©)
                if not goods_list:
                    sub_keys = []
                    if 'entity' in comp: sub_keys = list(comp['entity'].keys())
                    elif 'data' in comp: sub_keys = list(comp['data'].keys())
                    print(f"      No goods found. Sub-keys: {sub_keys}")

                if goods_list and isinstance(goods_list, list):
                    for g in goods_list:
                        # ë°ì´í„° ì†ŒìŠ¤ ê²°ì • (item_entity ë˜í¼ ëŒ€ì‘)
                        product_data = g
                        if 'item_entity' in g:
                            product_data = g['item_entity']
                        
                        # í•„ìˆ˜ í•„ë“œ ì¶”ì¶œ (snoê°€ ìƒí’ˆ ID)
                        # snoê°€ ì—†ìœ¼ë©´ item -> sno êµ¬ì¡°ì¼ ìˆ˜ë„ ìˆìŒ
                        if 'item' in product_data and isinstance(product_data['item'], dict):
                             product_data = product_data['item']
                             
                        p_id = product_data.get('sno')
                        p_name = product_data.get('name')
                        
                        if p_id and p_name:
                            # ê°€ê²© ì •ë³´: sale_priceê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ price
                            price = product_data.get('sale_price') or product_data.get('price') or 0
                            
                            # ì´ë¯¸ì§€ URL
                            p_image = product_data.get('image')
                            
                            captured_products.append({
                                'id': str(p_id),
                                'name': p_name,
                                'brand_name': product_data.get('market_name', 'Ably'),
                                'price': price,
                                'image': p_image,
                                'url': f"https://m.a-bly.com/goods/{p_id}"
                            })
                            
        except Exception as e:
            # print(f"     (Parsing error: {e})")
            pass

    # ì¤‘ë³µ ì œê±°
    unique_products = {p['id']: p for p in captured_products}.values()
    print(f"  âœ… API íŒŒì‹± ê²°ê³¼: ì´ {len(unique_products)}ê°œ ìƒí’ˆ ë°œê²¬")

    saved_count = 0
    for rank, item in enumerate(unique_products, start=1):
        if rank > 100: break
        if save_product_and_rank(item, rank, category["code"], category["name"]):
            saved_count += 1
            
    print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
    return saved_count


async def ably_crawl():
    start_time = datetime.now()
    print(f"[{start_time}] ì—ì´ë¸”ë¦¬(Ably) í¬ë¡¤ë§ ì‹œì‘...")
    log_crawl("running", {"message": "Started Ably crawl"})
    
    total_saved = 0
    
    async with async_playwright() as p:
        # ëª¨ë°”ì¼ ë·°í¬íŠ¸ ì„¤ì • (ì—ì´ë¸”ë¦¬ëŠ” ëª¨ë°”ì¼ ì›¹ ìµœì í™”)
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
            viewport={"width": 390, "height": 844},
            is_mobile=True,
            has_touch=True
        )
        page = await context.new_page()
        
        for category in TARGET_CATEGORIES:
            try:
                msg = await crawl_ably_category(page, category)
                total_saved += msg
            except Exception as e:
                print(f"  âŒ Error processing {category['name']}: {e}")
                
        await browser.close()
        
    duration = str(datetime.now() - start_time)
    print(f"[{datetime.now()}] í¬ë¡¤ë§ ì¢…ë£Œ. ì´ {total_saved}ê°œ ì €ì¥. ì†Œìš”ì‹œê°„: {duration}")
    log_crawl("completed", {
        "total_saved": total_saved, 
        "duration": duration
    })

if __name__ == "__main__":
    asyncio.run(ably_crawl())
