"""
SSG Department Store (ì‹ ì„¸ê³„ë°±í™”ì ) Best Ranking Crawler - SPA Version
URL: https://department.ssg.com/page/pc/ranking.ssg
"""
import os
import json
import time
import asyncio
import requests
from datetime import datetime
from translate_helper import get_english_brand
from dotenv import load_dotenv
from playwright.async_api import async_playwright

load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
SOURCE = "ssg"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("Error: SUPABASE_URL or SUPABASE_KEY not found.")
    exit(1)

HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=representation,resolution=merge-duplicates"
}

# ìƒˆ íƒ­ ì´ë¦„ ë§¤í•‘ (SPA í…ìŠ¤íŠ¸ -> DB Category Code)
CATEGORY_MAP = {
    "Beauty": {"code": "BEAUTY", "name": "ë·°í‹°"},
    "Fashion": {"code": "FASHION", "name": "íŒ¨ì…˜"},
    "Luxury": {"code": "LUXURY", "name": "ëª…í’ˆ"},
    "Kids": {"code": "KIDS", "name": "ìœ ì•„ë™"},
    "Sports": {"code": "SPORTS", "name": "ìŠ¤í¬ì¸ "},
    "Food & Life": {"code": "FOOD_LIFE", "name": "í‘¸ë“œ&ë¦¬ë¹™"},
}

def log_crawl(status, metadata=None):
    try:
        log_data = {
            "job_name": f"{SOURCE}_ranking_crawl",
            "status": status,
            "started_at": datetime.now().isoformat() if status == "running" else None,
            "finished_at": datetime.now().isoformat() if status in ("completed", "failed") else None,
            "metadata_json": metadata or {}
        }
        requests.post(f"{SUPABASE_URL}/rest/v1/crawl_logs", headers=HEADERS, json=log_data, timeout=10)
    except Exception as e:
        print(f"Warning: Could not log crawl status: {e}")

def save_product_and_rank(product_id, name, brand, price, image_url, url, rank, category_code, category_name):
    """products_masterì— upsert í›„, daily_rankings_v2ì— ë­í‚¹ ì €ì¥"""
    brand_en = get_english_brand(brand) if brand else ""

    product_record = {
        "product_id": str(product_id),
        "source": SOURCE,
        "name": name,
        "brand": brand or "",
        "brand_ko": brand or "",
        "brand_en": brand_en,
        "price": int(price) if price else None,
        "image_url": image_url,
        "url": url,
        "category": category_code, # ê²€ìƒ‰ìš© ì¶”ê°€
        "updated_at": datetime.now().isoformat()
    }

    res = requests.post(
        f"{SUPABASE_URL}/rest/v1/products_master",
        headers=HEADERS,
        params={"on_conflict": "source,product_id"},
        json=product_record,
        timeout=10
    )

    if res.status_code in [200, 201]:
        db_items = res.json()
        if db_items:
            internal_id = db_items[0].get("id")

            ranking_record = {
                "product_id": internal_id,
                "rank": rank,
                "date": datetime.now().date().isoformat(),
                "category_code": category_code,
                "source": SOURCE
            }

            rank_res = requests.post(
                f"{SUPABASE_URL}/rest/v1/daily_rankings_v2",
                headers=HEADERS,
                params={"on_conflict": "product_id,date,category_code"},
                json=ranking_record,
                timeout=10
            )
            if rank_res.status_code not in [200, 201]:
                print(f"  âš ï¸ Rank upsert error for {product_id}: {rank_res.text[:200]}")
            return True
    else:
        print(f"  âš ï¸ Product upsert error for {product_id}: {res.text[:200]}")
    return False

async def parse_products_from_dom(page):
    """SPA ë Œë”ë§ëœ DOM ìš”ì†Œì—ì„œ ìƒí’ˆ ì •ë³´ ì§ì ‘ ìŠ¤í¬ë˜í•‘"""
    products = await page.evaluate('''() => {
        const results = [];
        const cards = document.querySelectorAll('.template-grid-item');
        
        cards.forEach(card => {
            const rankEl = card.querySelector('.css-1k2hnaw');
            if(!rankEl) return;
            
            const brandEl = card.querySelector('.css-408eai');
            const nameEl = card.querySelector('.css-1mrk1dy');
            const priceEl = card.querySelector('.css-h9py3d');
            const imgEl = card.querySelector('img.loaded');
            const linkEl = card.querySelector('a[href*="itemId="]');
            
            if(nameEl && priceEl) {
                const href = linkEl ? linkEl.href : '';
                const itemIdMatch = href.match(/itemId=([^&]+)/);
                
                results.push({
                    rank: rankEl.innerText.replace(/[^0-9]/g, ''),
                    prdNm: nameEl.innerText.trim(),
                    brandNm: brandEl ? brandEl.innerText.trim() : '',
                    price: priceEl.innerText.replace(/[^0-9]/g, ''),
                    itemId: itemIdMatch ? itemIdMatch[1] : '',
                    imgUrl: imgEl ? imgEl.src : '',
                    prdUrl: href || `https://www.ssg.com/item/itemView.ssg?itemId=${itemIdMatch ? itemIdMatch[1] : ''}`
                });
            }
        });
        return results;
    }''')
    return products

async def ssg_crawl():
    start_time = datetime.now()
    print(f"[{start_time}] ì‹ ì„¸ê³„ë°±í™”ì (SPA) Best ë­í‚¹ í¬ë¡¤ë§ ì‹œì‘...")
    log_crawl("running", {"message": "Started SSG ranking crawl"})

    total_saved = 0
    total_errors = 0

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        page = await context.new_page()

        print("ğŸ” ë­í‚¹ í˜ì´ì§€ ì§„ì… ì¤‘...")
        await page.goto("https://department.ssg.com/page/pc/ranking.ssg", wait_until="networkidle", timeout=30000)
        await asyncio.sleep(4)

        buttons = await page.locator('[role="tablist"] button[role="tab"]').all()
        print(f"ì´ {len(buttons)}ê°œì˜ ì¹´í…Œê³ ë¦¬ íƒ­ ë°œê²¬.")

        for idx in range(len(buttons)):
            try:
                # DOM ë³€ê²½ì— ëŒ€ë¹„í•´ ë§¤ë²ˆ ìƒˆë¡œ íƒœê·¸ ìœ„ì¹˜ ì¡ê¸°
                current_buttons = await page.locator('[role="tablist"] button[role="tab"]').all()
                if idx >= len(current_buttons):
                    break
                    
                btn = current_buttons[idx]
                tab_text = await btn.inner_text()
                
                if tab_text not in CATEGORY_MAP:
                    print(f"â“ ë“±ë¡ë˜ì§€ ì•Šì€ íƒ­: {tab_text}, ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                cat_info = CATEGORY_MAP[tab_text]
                cat_code = cat_info["code"]
                cat_name_kr = cat_info["name"]
                
                print(f"\n--- [{cat_name_kr} ({cat_code})] í¬ë¡¤ë§ ì‹œì‘ ---")
                
                await btn.click()
                await asyncio.sleep(4) # SPA ë°ì´í„° ë Œë”ë§ ëŒ€ê¸°
                
                products = await parse_products_from_dom(page)
                print(f"  -> Extracted {len(products)} products")
                
                saved_count = 0
                for item in products:
                    if not item['prdNm'] or not item['itemId']:
                        continue
                        
                    ok = save_product_and_rank(
                        product_id=item['itemId'],
                        name=item['prdNm'],
                        brand=item['brandNm'],
                        price=item['price'],
                        image_url=item['imgUrl'],
                        url=item['prdUrl'],
                        rank=int(item['rank']),
                        category_code=cat_code,
                        category_name=cat_name_kr
                    )
                    if ok:
                        saved_count += 1
                
                print(f"  ğŸ’¾ [{cat_name_kr}] ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
                total_saved += saved_count
                
            except Exception as e:
                print(f"  âŒ [{tab_text}] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                total_errors += 1

        await browser.close()

    duration = str(datetime.now() - start_time)
    print(f"\n[{datetime.now()}] í¬ë¡¤ë§ ì¢…ë£Œ. ì´ {total_saved}ê°œ ì €ì¥, {total_errors}ê°œ ì‹¤íŒ¨. ì†Œìš”: {duration}")
    log_crawl("completed", {
        "total_saved": total_saved,
        "total_errors": total_errors,
        "duration": duration
    })

if __name__ == "__main__":
    asyncio.run(ssg_crawl())
