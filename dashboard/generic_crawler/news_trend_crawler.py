import os
import asyncio
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
import urllib.parse
from playwright.async_api import async_playwright
import local_ai_helper as ai
from config import SUPABASE_URL, HEADERS

# ENV Setup
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

CATEGORY = "News"

# RSSê°€ ëª¨ë‘ ë§‰í˜€ìˆì–´ ì „ë¶€ ì›¹ í¬ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ ì¬ì‘ì„±
WEB_SOURCES = [
    {
        "id": "apparelnews", 
        "name": "ì–´íŒ¨ëŸ´ë‰´ìŠ¤", 
        "url": "http://www.apparelnews.co.kr/news/news_list.php?mcode=m022vw10",
        "base_url": "http://www.apparelnews.co.kr",
        "link_keyword": "/news/news_view.php"
    },
    {
        "id": "cosinkorea", 
        "name": "ì½”ìŠ¤ì¸ì½”ë¦¬ì•„", 
        "url": "http://www.cosinkorea.com/news/articleList.html?sc_section_code=S1N1",
        "base_url": "http://www.cosinkorea.com",
        "link_keyword": "/news/article.html?no="
    },
    {
        "id": "beautynury", 
        "name": "ë·°í‹°ëˆ„ë¦¬", 
        "url": "http://www.beautynury.com/news/list/001002008",
        "base_url": "http://www.beautynury.com",
        "link_keyword": "/news/view/"
    },
    {
        "id": "fashionbiz", 
        "name": "íŒ¨ì…˜ë¹„ì¦ˆ", 
        "url": "http://www.fashionbiz.co.kr/main/",
        "base_url": "http://www.fashionbiz.co.kr",
        "link_keyword": "article.asp?idx="
    },
    {
        "id": "wkorea", 
        "name": "ë”ë¸”ìœ ì½”ë¦¬ì•„", 
        "url": "https://www.wkorea.com/category/fashion/",
        "base_url": "https://www.wkorea.com",
        "link_keyword": "wkorea.com/20"
    },
    {
        "id": "hwahae", 
        "name": "í™”í•´ ë¹„ì¦ˆë‹ˆìŠ¤", 
        "url": "https://business.hwahae.co.kr/insight/?utm_source=chatgpt.com",
        "base_url": "https://business.hwahae.co.kr",
        "link_keyword": "business.hwahae.co.kr/insight/blog/"
    }
]

def save_article_db(source_id, source_name, title, link, content):
    """DBì— ê¸°ì‚¬ë¥¼ ì €ì¥ (ë¡œì»¬ AI ë¶„ì„ í¬í•¨)"""
    try:
        # 1. DB ì¤‘ë³µ ì²´í¬ (URLì˜ ë§ˆì§€ë§‰ ìŠ¬ë˜ì‹œ ë’·ë¶€ë¶„ì´ë‚˜ íŒŒë¼ë¯¸í„° í™œìš©)
        unique_key = link.split('/')[-1].split('&')[0][:30]
        product_id = f"news_{source_id}_{unique_key}"
        
        check_res = requests.get(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={"product_id": f"eq.{product_id}", "select": "id"},
            timeout=10
        )
        if check_res.status_code == 200 and len(check_res.json()) > 0:
            return False

        print(f"  ğŸ¤– ì‹¤ì‹œê°„ ë¡œì»¬ AI ë‰´ìŠ¤ ë¶„ì„ ì¤‘: {title[:30]}...")
        
        # 2. ë¡œì»¬ AI ë¶„ì„ ì‹¤í–‰ (Mistral + Qwen)
        tags = ai.extract_article_tags(title, content)
        summary = ai.summarize_article(title, content)
        
        extracted_brand = tags.get("brand", source_name)
        if isinstance(extracted_brand, str) and extracted_brand.lower() == "null":
            extracted_brand = source_name
            
        print("  âœ¨ ë¶„ì„ ì™„ë£Œ. DB ì €ì¥ ì¤€ë¹„...")

        # 3. DB ì €ì¥
        product_record = {
            "product_id": product_id,
            "source": source_id,
            "name": title,
            "brand": extracted_brand,
            "price": 0,
            "image_url": "https://cdn-icons-png.flaticon.com/512/2965/2965879.png", 
            "url": link,
            "category": CATEGORY,
            "tags": tags,
            "ai_summary": {"insight": summary, "reason": f"ìˆ˜ì§‘: {source_name}"},
            "updated_at": datetime.now().isoformat()
        }
        
        res = requests.post(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={"on_conflict": "source,product_id"},
            json=product_record,
            timeout=10
        )
        return res.status_code in [200, 201]
    except Exception as e:
        print(f"  âŒ ë‰´ìŠ¤ ì €ì¥ ì—ëŸ¬ ({title[:20]}): {e}")
        return False

async def fetch_article_content(page, link):
    """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì— ë“¤ì–´ê°€ ë³¸ë¬¸ì„ ê¸ì–´ì˜´"""
    try:
        await page.goto(link, wait_until="domcontentloaded", timeout=30000)
        await asyncio.sleep(1) # JS ì—°ì‚° ëŒ€ê¸°
        
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        
        # ì“¸ë°ì—†ëŠ” íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼)
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()
            
        # ë³¸ë¬¸ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œæœ€ã‚‚ ë‚´ìš©ì´ ë§ì€ ë¸”ë¡ì„ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼ (ê°„ì´ íœ´ë¦¬ìŠ¤í‹±)
        paragraphs = soup.find_all(['p', 'div'])
        longest_text = ""
        
        for p in paragraphs:
            text = p.get_text(separator=" ", strip=True)
            if len(text) > len(longest_text):
                longest_text = text
                
        # ì“¸ë°ì—†ì´ ê¸´ ê²½ìš°(ë°°ë„ˆ ì§‘í•©)ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì–´ëŠ ì •ë„ ê¸¸ì´(ì˜ˆ: 300ì) ì´ìƒì´ë©´ ë³¸ë¬¸ ì·¨ê¸‰
        return longest_text[:3000] # AI ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤ ìµœëŒ€ 3000ì ì œí•œ
        
    except Exception as e:
        print(f"  âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({link}): {e}")
        return ""

async def crawl_web_source(context, source):
    print(f"\n--- [{source['name']}] ì›¹ í¬ë¡¤ë§ ì‹œë„ ---")
    page = await context.new_page()
    total_saved = 0
    try:
        await page.goto(source['url'], wait_until="domcontentloaded", timeout=60000)
        await asyncio.sleep(2)
        
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        
        all_links = soup.find_all('a')
        
        # URL ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ê¸°ì‚¬ ë§í¬ë§Œ í•„í„°ë§
        valid_articles = []
        seen_urls = set()
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # 1. ê³ ìœ  í‚¤ì›Œë“œê°€ í¬í•¨ëœ href ì¸ê°€?
            # 2. ì´ë¯¸ ë“±ë¡ëœ URLì´ ì•„ë‹Œê°€? 
            # 3. í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ê¸°ì‚¬ ì œëª©ë‹µê²Œ ê¸´ê°€? (> 10ì)
            if source['link_keyword'] in href and href not in seen_urls and len(text) > 10:
                full_url = urllib.parse.urljoin(source['base_url'], href)
                valid_articles.append({"title": text, "link": full_url})
                seen_urls.add(href)
                
            if len(valid_articles) >= 3: # ê° ë§¤ì²´ë‹¹ ìµœì‹  3ê°œë§Œ (AI ë¦¬ì†ŒìŠ¤ ì¡°ì ˆ)
                break
                
        print(f"  ğŸ‘‰ ë°œê²¬ëœ ìœ íš¨ ê¸°ì‚¬ ìˆ˜: {len(valid_articles)}ê°œ")
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ë° AI ë¶„ì„ í›„ ì €ì¥
        for article in valid_articles:
            # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            content = await fetch_article_content(page, article['link'])
            if len(content) < 50:
                content = article['title'] # ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œëª©ì´ë¼ë„ ë„˜ê¹€
                
            if save_article_db(source['id'], source['name'], article['title'], article['link'], content):
                 total_saved += 1
                 
        if total_saved > 0:
            print(f"  âœ… {total_saved}ê°œ ê¸°ì‚¬ ì‹ ê·œ ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ")
        else:
            print("  â„¹ï¸ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ (ë˜ëŠ” ëª¨ë‘ ì €ì¥ ì‹¤íŒ¨)")
                 
    except Exception as e:
        print(f"  âŒ ì—ëŸ¬: {e}")
    finally:
        await page.close()
        
    return total_saved

async def main():
    start_time = datetime.now()
    print(f"========== ë‰´ìŠ¤ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ (Web & AI) ì‹œì‘ ({start_time}) ==========")
    total_saved = 0
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=['--disable-blink-features=AutomationControlled']
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ (ì†ë„ 2~3ë°° í–¥ìƒ)
        tasks = [crawl_web_source(context, source) for source in WEB_SOURCES]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for res in results:
            if isinstance(res, int):
                total_saved += res
            else:
                print(f"  âŒ ë³‘ë ¬ ì²˜ë¦¬ ì—ëŸ¬: {res}")
                
        await browser.close()
        
    duration = str(datetime.now() - start_time)
    print(f"\n========== ë‰´ìŠ¤ í¬ë¡¤ë§ ì¢…ë£Œ. ì´ {total_saved}ê°œ ì €ì¥. ì†Œìš”ì‹œê°„: {duration} ==========")

if __name__ == "__main__":
    asyncio.run(main())
