import os
import time
import pandas as pd
import requests
from datetime import datetime
from dotenv import load_dotenv
from pytrends.request import TrendReq
import local_ai_helper as ai
from config import SUPABASE_URL, HEADERS

# ENV Setup
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))

SOURCE = "google_trends"

# Shopping/E-commerce seed keywords to find rising trends
SHOPPING_KEYWORDS = [
    # ê¸°ì¡´ ì‡¼í•‘ ì¹´í…Œê³ ë¦¬
    "í™”ì¥í’ˆ", "ì›í”¼ìŠ¤", "ê°€ë°©", "ìŠ¤ë‹ˆì»¤ì¦ˆ", "ë·°í‹°",
    "í–¥ìˆ˜", "í¬ë¡œìŠ¤ë°±", "ìŠ¤í‚¨ì¼€ì–´", "ë¦½ìŠ¤í‹±", "ì¿ ì…˜íŒŒë°",
    "ìš´ë™í™”", "ë§¨íˆ¬ë§¨", "íŒ¨ë”©", "ì²­ë°”ì§€", "ìŠ¬ë™ìŠ¤",
    "ê°€ë””ê±´", "ë‹ˆíŠ¸", "ë¸”ë¼ìš°ìŠ¤", "ê·€ê±¸ì´", "ëª©ê±¸ì´",
    "ì„ í¬ë¦¼", "í‹´íŠ¸", "ë‹¤ì´ì–´íŠ¸", "ë ˆê¹…ìŠ¤", "ì‡¼í•‘",
    # í™”ì¥í’ˆ ì„±ë¶„ (íŠ¸ë Œë“œ ì„±ë¶„ íŒŒì•…ìš©)
    "íˆì•Œë£¨ë¡ ì‚°", "ì½œë¼ê²", "ë ˆí‹°ë†€", "ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ", "ì„¸ë¼ë§ˆì´ë“œ",
    "ë¹„íƒ€ë¯¼C ì„¸ëŸ¼", "PDRN", "íŒí…Œë†€", "ê¸€ë£¨íƒ€ì¹˜ì˜¨", "ì¤„ê¸°ì„¸í¬",
]

def log_crawl(status, metadata=None):
    try:
        log_data = {
            "job_name": f"{SOURCE}_ranking_crawl",
            "status": status,
            "started_at": datetime.now().isoformat() if status == "running" else None,
            "finished_at": datetime.now().isoformat() if status in ("completed", "failed") else None,
            "metadata_json": metadata or {}
        }
        requests.post(f"{SUPABASE_URL}/rest/v1/crawl_logs", headers=HEADERS, json=log_data, timeout=10)
    except Exception as e:
        print(f"Warning: Could not log crawl status: {e}")

def save_keyword_trend(keyword, rank, spike_value):
    try:
        product_id = f"kw_gt_{keyword}"

        # 1. ê¸°ì¡´ ë°ì´í„° ë¨¼ì € ì¡°íšŒ (AI ë¶„ì„ ì—¬ë¶€ í™•ì¸)
        existing_res = requests.get(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={"product_id": f"eq.{product_id}", "select": "id,tags,ai_summary"},
            timeout=10
        )
        existing = existing_res.json() if existing_res.status_code == 200 else []
        already_analyzed = existing and existing[0].get("ai_summary") and existing[0].get("tags")

        # 2. AI ë¶„ì„ì€ ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
        if already_analyzed:
            tags = existing[0].get("tags", {})
            insight = existing[0].get("ai_summary", {})
            print(f"  âš¡ AI ë¶„ì„ ìºì‹œ ì‚¬ìš©: {keyword} (API ì ˆì•½)")
        else:
            print(f"  ğŸ¤– ìµœì´ˆ AI ë¶„ì„ ì‹¤í–‰: {keyword}")
            tags = ai.extract_tags(keyword)
            insight = ai.generate_insight(keyword, SOURCE)
            print("  âœ¨ ë¶„ì„ ì™„ë£Œ. DB ì €ì¥ ì¤€ë¹„...")

        product_record = {
            "product_id": product_id,
            "source": SOURCE,
            "name": keyword,
            "brand": f"Google ê¸‰ìƒìŠ¹ (+{spike_value}%)",
            "price": 0,
            "image_url": "https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_92x30dp.png",
            "url": f"https://www.google.com/search?tbm=shop&q={keyword}",
            "category": "Shopping",
            "tags": tags,
            "ai_summary": insight,
            "updated_at": datetime.now().isoformat()
        }
        res = requests.post(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={"on_conflict": "source,product_id"},
            json=product_record,
            timeout=10
        )
        
        if res.status_code in [200, 201]:
            db_items = res.json()
            if db_items:
                internal_id = db_items[0].get("id")
                # 2. Upsert to daily_rankings_v2
                ranking_record = {
                    "product_id": internal_id,
                    "rank": rank,
                    "date": datetime.now().date().isoformat(),
                    "category_code": "google",
                    "source": SOURCE
                }
                requests.post(
                    f"{SUPABASE_URL}/rest/v1/daily_rankings_v2",
                    headers=HEADERS,
                    params={"on_conflict": "product_id,date,category_code"},
                    json=ranking_record,
                    timeout=10
                )
                return True
        return False
    except Exception as e:
        print(f"  âŒ Save error for {keyword}: {e}")
        return False

def get_rising_trends():
    print("ì´ˆê¸°í™”: Google Trends API (Pytrends)...")
    pytrend = TrendReq(hl='ko-KR', tz=-540)
    
    all_rising = {}
    
    chunk_size = 5
    for i in range(0, len(SHOPPING_KEYWORDS), chunk_size):
        chunk = SHOPPING_KEYWORDS[i:i+chunk_size]
        print(f"ğŸ” íƒìƒ‰ ì¤‘: {chunk}")
        
        try:
            # ì¹´í…Œê³ ë¦¬ 18 = ì‡¼í•‘ (Shopping)
            pytrend.build_payload(chunk, cat=18, geo='KR', timeframe='now 7-d')
            rq = pytrend.related_queries()
            
            if rq:
                for kw in chunk:
                    if kw in rq and rq[kw] and rq[kw]['rising'] is not None:
                        df = rq[kw]['rising']
                        for _, row in df.iterrows():
                            query_str = row['query']
                            value = row['value']
                            if query_str not in all_rising or value > all_rising[query_str]:
                                all_rising[query_str] = value
        except Exception as e:
            print(f"  âš ï¸ Pytrends ì˜¤ë¥˜ ({chunk}): {e}")
            
        time.sleep(3) # ë ˆì´íŠ¸ ë¦¬ë°‹ ìš°íšŒ ëŒ€ê¸°
        
    sorted_trends = sorted(all_rising.items(), key=lambda x: x[1], reverse=True)
    return sorted_trends[:50]

def google_trends_crawl():
    start_time = datetime.now()
    print(f"[{start_time}] êµ¬ê¸€ íŠ¸ë Œë“œ (ì‡¼í•‘ íŠ¹í™”) í¬ë¡¤ë§ ì‹œì‘...")
    log_crawl("running", {"message": "Started Google Trends shopping focused crawl"})

    try:
        trends = get_rising_trends()
        print(f"  âœ… {len(trends)}ê°œ ì£¼ìš” ì‡¼í•‘ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ ë°œê²¬")
        
        saved_count = 0
        for rank, (query_str, val) in enumerate(trends, start=1):
            if save_keyword_trend(query_str, rank, val):
                saved_count += 1
                
        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
        log_crawl("completed", {"total_saved": saved_count, "duration": str(datetime.now() - start_time)})

    except Exception as e:
        print(f"  âŒ êµ¬ê¸€ íŠ¸ë Œë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        log_crawl("failed", {"error": str(e)})

if __name__ == "__main__":
    google_trends_crawl()
