import os
import json
import requests
from collections import Counter
from datetime import datetime, timedelta
from dotenv import load_dotenv
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from generic_crawler.config import SUPABASE_URL, HEADERS

# ENV Setup
load_dotenv(os.path.join(os.path.dirname(os.path.abspath(__file__)), ".env"))

def fetch_recent_news_tags():
    """ìµœê·¼ 48ì‹œê°„ ë‚´ì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íƒœê·¸(brand, ingredient, fashion_style)ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    two_days_ago = (datetime.now() - timedelta(days=2)).isoformat()
    
    try:
        res = requests.get(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers=HEADERS,
            params={
                "category": "eq.News",
                "created_at": f"gte.{two_days_ago}",
                "select": "tags"
            },
            timeout=30
        )
        res.raise_for_status()
        return res.json()
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

STOPWORDS = {
    "í™”ì¥í’ˆ", "ë·°í‹°", "íŒ¨ì…˜", "ë¸Œëœë“œ", "ì‹ ì œí’ˆ", "ì¶œì‹œ", "í”„ë¡œëª¨ì…˜", "ì´ë²¤íŠ¸", 
    "ìŠ¤í‚¨ì¼€ì–´", "ë©”ì´í¬ì—…", "ì•„ì´í…œ", "ì»¬ë ‰ì…˜", "ìº í˜ì¸", "íŠ¸ë Œë“œ", "ìŠ¤íƒ€ì¼", "ì„±ë¶„"
}

def is_valid_term(term):
    """ë¶ˆìš©ì–´ í•„í„°ë§"""
    if not term or len(term) < 2:
        return False
    if term in STOPWORDS:
        return False
    return True

def aggregate_trends(records):
    """íƒœê·¸ ë°°ì—´ì„ ìˆœíšŒí•˜ë©° ë¸Œëœë“œì™€ ì„±ë¶„ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤."""
    brands = Counter()
    ingredients = Counter()
    styles = Counter()
    
    for record in records:
        tags = record.get("tags")
        if not tags or not isinstance(tags, dict):
            continue
            
        # ë¸Œëœë“œ ì§‘ê³„ (ì½¤ë§ˆë¡œ ë¶„ë¦¬ëœ ê²½ìš° ì²˜ë¦¬)
        if "brand" in tags and isinstance(tags["brand"], str):
            b_list = [b.strip() for b in tags["brand"].split(",") if is_valid_term(b.strip().lower()) and b.strip().lower() != "null"]
            brands.update(b_list)
            
        # ì„±ë¶„ ì§‘ê³„
        if "ingredient" in tags and isinstance(tags["ingredient"], str):
            i_list = [i.strip() for i in tags["ingredient"].split(",") if is_valid_term(i.strip().lower()) and i.strip().lower() != "null"]
            ingredients.update(i_list)
            
        # íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ì§‘ê³„
        if "fashion_style" in tags and isinstance(tags["fashion_style"], str):
            s_list = [s.strip() for s in tags["fashion_style"].split(",") if is_valid_term(s.strip().lower()) and s.strip().lower() != "null"]
            styles.update(s_list)
            
    return brands, ingredients, styles

def save_daily_insight(brands, ingredients, styles, analyzed_count):
    """ì§‘ê³„ëœ ê²°ê³¼ë¥¼ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë ˆì½”ë“œë¡œ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    # ìƒìœ„ 5ê°œì”©ë§Œ ì¶”ì¶œ
    top_brands = [f"{k}({v})" for k, v in brands.most_common(5)]
    top_ing = [f"{k}({v})" for k, v in ingredients.most_common(5)]
    top_styles = [f"{k}({v})" for k, v in styles.most_common(5)]
    
    insight_text = f"ğŸ“° ìµœê·¼ 48ì‹œê°„ ë™ì•ˆ {analyzed_count}ê°œì˜ ë·°í‹°/íŒ¨ì…˜ ë‰´ìŠ¤ê°€ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
    if top_brands: insight_text += f"- **ê°€ì¥ í•«í•œ ë¸Œëœë“œ**: {', '.join(top_brands)}\n"
    if top_ing: insight_text += f"- **ì£¼ëª©ë°›ëŠ” ì„±ë¶„**: {', '.join(top_ing)}\n"
    if top_styles: insight_text += f"- **ë– ì˜¤ë¥´ëŠ” ìŠ¤íƒ€ì¼**: {', '.join(top_styles)}"
    
    print(f"\n--- ì˜¤ëŠ˜ì˜ ì¢…í•© ë¶„ì„ ---\n{insight_text}\n")
    
    record = {
        "product_id": f"daily_insight_{today_str}",
        "source": "AI_Aggregator",
        "name": f"{today_str} ë·°í‹°/íŒ¨ì…˜ ì¢…í•© í†µê³„",
        "brand": "System",
        "price": 0,
        "image_url": "https://cdn-icons-png.flaticon.com/512/3076/3076332.png", # í†µê³„ ì•„ì´ì½˜
        "url": "https://dashboard.local",
        "category": "Daily Insight",
        "ai_summary": {"insight": insight_text, "reason": "ì¼ì¼ ë‰´ìŠ¤ ì¢…í•© ë°ì´í„°"},
        "tags": {"top_brands": dict(brands.most_common(5)), "top_ingredients": dict(ingredients.most_common(5))}
    }
    
    try:
        requests.post(
            f"{SUPABASE_URL}/rest/v1/products_master",
            headers={**HEADERS, "Prefer": "return=representation,resolution=merge-duplicates"},
            params={"on_conflict": "source,product_id"},
            json=record,
            timeout=10
        )
        print("âœ… ì¼ì¼ íŠ¸ë Œë“œ ìš”ì•½ ì €ì¥ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print(f"[{datetime.now()}] íŠ¸ë Œë“œ ì¢…í•© ì¹´ìš´í„° ì‹œì‘...")
    records = fetch_recent_news_tags()
    
    if not records:
        print("  âš ï¸ ë¶„ì„í•  ë‰´ìŠ¤ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"  ğŸ” {len(records)}ê°œì˜ ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ë°œê²¬.")
        b, i, s = aggregate_trends(records)
        save_daily_insight(b, i, s, len(records))
